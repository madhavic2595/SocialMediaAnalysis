{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need the basic sklearn, numpy, pandas. Upload the train and test csv files in the same folder as the notebook. Or if using colab, upload these 2 files in the folders section. Run the cells in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytz\n",
    "!pip3 install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the required header files below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, pytz\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will read our data CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the preprocessing functions that we have developed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Function written for UTC Time\n",
    "tz_l = pytz.all_timezones\n",
    "def processF(text): \n",
    "    text = str(text)\n",
    "    backup = text\n",
    "    if text == \"nan\":\n",
    "        return text\n",
    "    #text = text.replace(',', '')\n",
    "    words = text.split()\n",
    "    \n",
    "    finalWord = \"\"\n",
    "    #Look for singular words, like Mumbai, Argentina \n",
    "    charr = \"/\"\n",
    "    if len(words) == 1 and charr not in words:\n",
    "        for i in tz_l:\n",
    "            orig = i\n",
    "            i = i.lower()\n",
    "            if words[0] in i: \n",
    "                finalWord = orig\n",
    "                #print(\"Word in consideration is\", words[0])\n",
    "                #print(\"Returned Word is\", finalWord)\n",
    "                #print(\"returned values are:\", orig)\n",
    "                return orig\n",
    "    if len(words) > 1:\n",
    "        backup = backup.replace(',', '')\n",
    "        backup = backup.replace('/', '')\n",
    "        words = backup.split()\n",
    "        for x in words:\n",
    "            for i in tz_l: \n",
    "                orig = i\n",
    "                i = i.lower()\n",
    "                if x in i:\n",
    "                    #print(\"Word in consideration is\", x)\n",
    "                    #print(\"Returned Word is\", i)\n",
    "                    finalWord = orig\n",
    "                    #print(\"Original and returned values are:\", x, orig)\n",
    "                    return orig\n",
    "    if len(words) > 0:\n",
    "        if charr in words[0] and len(words) == 1:\n",
    "            wordStr = str(words)\n",
    "            wordStr = wordStr.replace('/', ' ')\n",
    "            wordStr = wordStr.replace(',', ' ')\n",
    "            ww = wordStr.split()\n",
    "            #print(\"Split shit is\", ww)\n",
    "            for x in ww:\n",
    "                for i in tz_l:\n",
    "                    orig = i \n",
    "                    i = i.lower()\n",
    "                    if x in i:\n",
    "                        finalWord = orig\n",
    "                        #print(\"Original and returned values are:\", x, orig)\n",
    "                        return orig\n",
    "                  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocess(text):\n",
    "    if text == \"nan\":\n",
    "        return text\n",
    "    for i in tz_l: \n",
    "        orig = i\n",
    "        i = i.lower()\n",
    "        text = text.replace(',', '')\n",
    "        text = text.lower()\n",
    "        text_list = text.split()\n",
    "        for j in text_list:\n",
    "            if j in i:\n",
    "                return orig\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utcprocess(strval):\n",
    "    if strval == \"nan\":\n",
    "        return strval\n",
    "    if str(strval) in tz_l:\n",
    "        return datetime.datetime.utcnow().replace(tzinfo=pytz.utc).astimezone(pytz.timezone(strval)).utcoffset().total_seconds()\n",
    "    else: \n",
    "        return strval\n",
    "    return strval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is the main pre-processing function. This does all the skew removals, transforms, scalings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_nan_adjusted):\n",
    "    \n",
    "    #find the presence of URL\n",
    "    data_nan_adjusted['url_present'] = data_nan_adjusted[\"Personal URL\"].isna().astype(int)\n",
    "    \n",
    "    #Extract information from date\n",
    "    data_nan_adjusted[\"Date Created\"] = pd.to_datetime(data_nan_adjusted[\"Profile Creation Timestamp\"])\n",
    "    #extract months and year info from profile date created\n",
    "    data_nan_adjusted[\"Year Created\"] = data_nan_adjusted[\"Date Created\"].dt.year\n",
    "    data_nan_adjusted[\"Months\"] = data_nan_adjusted[\"Date Created\"].dt.to_period('M').astype(int)\n",
    "    # Converting time stamp to the age of the account: 2021 - year_of_creation\n",
    "    data_nan_adjusted['Profile Creation Timestamp'] = data_nan_adjusted['Profile Creation Timestamp'].\\\n",
    "                                    apply(lambda x: 2021 - int(x.split(\" \")[-1]))\n",
    "    \n",
    "    #fill na values of color\n",
    "    data_nan_adjusted['Profile Text Color'].fillna('ffffff', inplace=True)\n",
    "    data_nan_adjusted['Profile Page Color'].fillna('ffffff', inplace=True)\n",
    "    data_nan_adjusted['Profile Theme Color'].fillna('ffffff', inplace=True)\n",
    "    \n",
    "    \n",
    "    #frequency encoding for colors -> Performs the best for the columns. \n",
    "    enc_nom_1 = (data_nan_adjusted.groupby('Profile Text Color').size()) / len(data_nan_adjusted)\n",
    "    enc_nom_2 = (data_nan_adjusted.groupby('Profile Page Color').size()) / len(data_nan_adjusted)\n",
    "    enc_nom_3 = (data_nan_adjusted.groupby('Profile Theme Color').size()) / len(data_nan_adjusted)\n",
    "    data_nan_adjusted['col1_encode'] = data_nan_adjusted['Profile Text Color'].apply(lambda x : enc_nom_1[x])\n",
    "    data_nan_adjusted['col2_encode'] = data_nan_adjusted['Profile Page Color'].apply(lambda x : enc_nom_2[x])\n",
    "    data_nan_adjusted['col3_encode'] = data_nan_adjusted['Profile Theme Color'].apply(lambda x : enc_nom_3[x])\n",
    "    \n",
    "    #fill na of Profile Cover Image Status based on profile view size customized\n",
    "    data_nan_adjusted[\"Profile Cover Image Status\"] = data_nan_adjusted[\"Profile Cover Image Status\"].\\\n",
    "                                fillna(value=data_nan_adjusted['Is Profile View Size Customized?'])\n",
    "    \n",
    "    #replace it with numbers to find the correlation\n",
    "    data_nan_adjusted[\"Profile Cover Image Status\"].replace({\"Not set\":0 , \"Set\":1}, inplace=True)\n",
    "    data_nan_adjusted[\"Profile Cover Image Status\"] = data_nan_adjusted[\"Profile Cover Image Status\"].astype(int)\n",
    "\n",
    "    #fill nan by median after group by profile categories\n",
    "    data_nan_adjusted['Avg Daily Profile Clicks'] = data_nan_adjusted['Avg Daily Profile Clicks']\\\n",
    "        .fillna(data_nan_adjusted.groupby(['Profile Category'])['Avg Daily Profile Clicks'].transform('median'))\n",
    "\n",
    "    #fill median - this has more correlation with likes\n",
    "    data_nan_adjusted['Avg Daily Profile Visit Duration in seconds']\\\n",
    "    .fillna(data_nan_adjusted['Avg Daily Profile Visit Duration in seconds'].median(), inplace=True)\n",
    "\n",
    "    #onehot encoding for Profile category\n",
    "    data_nan_adjusted['Profile Category'] = data_nan_adjusted['Profile Category'].str.replace(\" \",'unknown')\n",
    "    data_nan_adjusted = pd.concat([data_nan_adjusted, pd.get_dummies(data_nan_adjusted['Profile Category'], \\\n",
    "                                                                     dtype=int, drop_first=True)], axis=1)\n",
    "    \n",
    "    #label encoding for Profile Verification Status and Location Public Visibility\n",
    "    data_nan_adjusted[\"Profile Verification Status\"].replace({\"Not verified\": 0 , \"Verified\": 1, \"Pending\":0}, inplace=True)\n",
    "    data_nan_adjusted[\"Location Public Visibility\"].replace({\"??\": 0, 'Disabled': 1, 'Enabled': 2, 'disabled': 1, 'enabled': 2},\\\n",
    "                                                            inplace=True)\n",
    "\n",
    "    data_nan_adjusted[\"Is Profile View Size Customized?\"] = data_nan_adjusted[\"Is Profile View Size Customized?\"].astype(int)\n",
    "    \n",
    "    \n",
    "    #log transformations -> fixes skews\n",
    "    data_nan_adjusted['Num of People Following log'] = np.log(1+data_nan_adjusted['Num of People Following'])\n",
    "    data_nan_adjusted['Num of Followers log'] = np.log(1+data_nan_adjusted['Num of Followers'])\n",
    "    data_nan_adjusted['Num of Status Updates log'] = np.log(1+data_nan_adjusted['Num of Status Updates'])\n",
    "    data_nan_adjusted['Num of Direct Messages log'] = np.log(1+data_nan_adjusted['Num of Direct Messages'])\n",
    "    data_nan_adjusted['Num of Direct Messages log'] = np.log(1+data_nan_adjusted['Num of Direct Messages'])\n",
    "    \n",
    "    #one hot encoding for user language\n",
    "    data_nan_adjusted = pd.concat([data_nan_adjusted, pd.get_dummies(data_nan_adjusted['User Language'].\\\n",
    "                                                                     str.lower(), dtype=int, drop_first=True)], axis=1)\n",
    "    \n",
    "    #robust scalar for profile clicks robust to remove outliers and log transformation\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25, 75)) \n",
    "    data_nan_adjusted[['Avg Daily Profile Clicks robust']] = np.log(1+scaler.\\\n",
    "                                            fit_transform(data_nan_adjusted[['Avg Daily Profile Clicks']]))\n",
    "    \n",
    "    #Quantile transformation for Daily Profile Visit Duration -> makes the shape bell curve esque\n",
    "    Visit_duration_transform = preprocessing.QuantileTransformer(output_distribution='normal')\n",
    "    data_nan_adjusted[['Avg Daily Profile Visit Duration in seconds robust']] = Visit_duration_transform.\\\n",
    "                                fit_transform(data_nan_adjusted[['Avg Daily Profile Visit Duration in seconds']])\n",
    "    \n",
    "    data_nan_adjusted['Location'] =  data_nan_adjusted['Location'].apply(processF)\n",
    "    data_nan_adjusted['Location'] =  data_nan_adjusted['Location'].apply(simple_preprocess)\n",
    "\n",
    "    data_nan_adjusted['UTC Offset'].fillna(data_nan_adjusted['UTC Offset'].mean(), inplace = True)\n",
    "    \n",
    "    #Transform improves correlation the most\n",
    "    data_nan_adjusted[['UTC Offset']] = scaler.fit_transform(data_nan_adjusted[['UTC Offset']])\n",
    "    \n",
    "    return data_nan_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns based on correaltion with num of likes. This improves regession output\n",
    "def drop_cols(data_nan_adjusted1, prevVar, test = False):\n",
    "    data_nan_adjusted1 = prevVar.copy()\n",
    "    data_nan_adjusted1.drop(['Personal URL','Id', 'User Name', 'Personal URL', 'Year Created', 'Date Created', 'Profile Image'], axis=1, inplace=True)\n",
    "    data_nan_adjusted1.drop(['Location', 'User Language', 'User Time Zone'], axis=1, inplace=True)\n",
    "    data_nan_adjusted1.drop(['Profile Text Color', 'Profile Page Color', 'Profile Theme Color'], axis=1, inplace=True)\n",
    "    data_nan_adjusted1.drop(['Profile Category', 'Num of People Following', 'Num of Followers', 'Num of Status Updates', 'Num of Direct Messages' ], axis=1, inplace=True)\n",
    "    data_nan_adjusted1.drop(['Avg Daily Profile Visit Duration in seconds', 'Avg Daily Profile Clicks'], axis=1, inplace=True)\n",
    "    \n",
    "    if test:\n",
    "        data_nan_adjusted1.drop(['pt', 'es', 'en-gb', 'ru', 'it', 'nl', 'ca',\n",
    "       'de', 'fr', 'id', 'fi', 'he', 'pl', 'no', 'ro', 'th',\n",
    "       'zh-cn', 'cs', 'sv', 'hu'], axis=1, inplace=True)\n",
    "    else:\n",
    "        data_nan_adjusted1.drop(['ca', 'cs', 'da', 'de', 'el', 'en-gb', 'es', 'fi', 'fr', 'hu', 'id', 'it', 'nl', 'pl','ru', 'pt', 'sk', 'sr', 'sv', 'th', 'uk', 'zh-tw', 'zh-cn'], axis=1, inplace=True)\n",
    "    \n",
    "    return data_nan_adjusted1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(data_nan_adjusted1):\n",
    "    features = data_nan_adjusted1[data_nan_adjusted1.columns.difference(['Num of Profile Likes'])] \n",
    "    scaler = preprocessing.StandardScaler() \n",
    "    minmax_df = scaler.fit_transform(features)\n",
    "    return minmax_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavithra\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:1091: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "data_nan_adjusted = preprocess(data.copy())\n",
    "data_nan_adjusted1 = drop_cols(data_nan_adjusted.copy(), data_nan_adjusted, False)\n",
    "minmax_df = scaling(data_nan_adjusted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking log of labels since it is skewed\n",
    "labels = np.log(data_nan_adjusted1['Num of Profile Likes'] + 1)\n",
    "features = data_nan_adjusted1[data_nan_adjusted1.columns.difference(['Num of Profile Likes'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model defined for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "     ('bag svr', BaggingRegressor(SVR(kernel = 'rbf', C=1), random_state = 0)),\n",
    "     ('adaboost', AdaBoostRegressor(n_estimators=100, random_state = 0)),\n",
    "     ('xgb', xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 4, alpha = 10, random_state = 0)),\n",
    "    ('ridge', Ridge(normalize = False, alpha = 5.0)),\n",
    "    ('rf', RandomForestRegressor(oob_score = True, random_state = 0))\n",
    "]\n",
    "\n",
    "reg = StackingRegressor(estimators=estimators, final_estimator=BaggingRegressor(SVR(kernel = 'linear', C=1), n_estimators=50, random_state = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation strategy used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_m value is for test 1.7108834327657283\n",
      "Error_m value is for test 1.7364874684993097\n",
      "Error_m value is for test 1.713054421797459\n",
      "Error_m value is for test 1.735657993947408\n",
      "Overall K-Fold CV RMSLE is 1.7240208292524761\n"
     ]
    }
   ],
   "source": [
    "####K FOLD CROSS VALIDATION\n",
    "sumCount = 0\n",
    "count = 0\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(minmax_df):\n",
    "    X_train, X_test = minmax_df[train_index], minmax_df[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = np.floor(np.exp(reg.predict(X_test))) - 1\n",
    "    test_y_exp = np.exp(y_test) - 1\n",
    "    y_pred[y_pred<0] = 0\n",
    "    err_m = np.sqrt(mean_squared_log_error(y_pred, test_y_exp))\n",
    "    sumCount = sumCount + err_m\n",
    "    count += 1\n",
    "    \n",
    "    print(\"Error_m value is for test\", err_m)\n",
    "print(\"Overall K-Fold CV RMSLE is\", (sumCount / count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for final prediction of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingRegressor(estimators=[('bag svr',\n",
       "                               BaggingRegressor(base_estimator=SVR(C=1))),\n",
       "                              ('adaboost', AdaBoostRegressor(n_estimators=100)),\n",
       "                              ('xgb',\n",
       "                               XGBRegressor(alpha=10, base_score=None,\n",
       "                                            booster=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.3, gamma=None,\n",
       "                                            gpu_id=None, importance_type='gain',\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rate=0.1,\n",
       "                                            m...\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            random_state=None, reg_alpha=None,\n",
       "                                            reg_lambda=None,\n",
       "                                            scale_pos_weight=None,\n",
       "                                            subsample=None, tree_method=None,\n",
       "                                            validate_parameters=None,\n",
       "                                            verbosity=None)),\n",
       "                              ('ridge', Ridge(alpha=5.0)),\n",
       "                              ('rf', RandomForestRegressor(oob_score=True))],\n",
       "                  final_estimator=BaggingRegressor(base_estimator=SVR(C=1,\n",
       "                                                                      kernel='linear'),\n",
       "                                                   n_estimators=50))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2 = StackingRegressor(estimators=estimators, final_estimator=BaggingRegressor(SVR(kernel = 'linear', C=1), n_estimators=50))\n",
    "reg2.fit(minmax_df, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavithra\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:1091: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "test_d = pd.read_csv('test.csv')\n",
    "\n",
    "#preprocess test data\n",
    "data_nan_adjusted_t = preprocess(test_d.copy())\n",
    "data_nan_adjusted1_t = drop_cols(data_nan_adjusted_t.copy(), data_nan_adjusted_t, True)\n",
    "data_nan_adjusted1_t_array = scaling(data_nan_adjusted1_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and Generate submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict num of likes and create Submission.csv\n",
    "y_pred_2 = np.floor(np.exp(reg2.predict(data_nan_adjusted1_t_array))) - 1\n",
    "y_pred_2[y_pred_2 < 0] = 0\n",
    "pd.set_option('display.max_columns', None)\n",
    "t_dataframe=pd.DataFrame(y_pred_2, columns=['predicted'])\n",
    "t_dataframe[\"Id\"] = test_d.Id\n",
    "columns_titles = [\"Id\",\"predicted\"]\n",
    "t_dataframe=t_dataframe.reindex(columns=columns_titles)\n",
    "t_dataframe.to_csv(\"Submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the file needed for submission is generated. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
